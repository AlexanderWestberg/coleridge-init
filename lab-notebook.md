
<h6> 9/6-21 </h6>
- Began to understand the basics of the data

- Made some papernotes of some of the basis question objectives of the project
	- Can NLP find the hidden-in-plain data citations?
	- Can ML find the link between the words used in the research atricles and the data reference in the articele?
- We want an automated way of finding out what datasets are being used to solve probles, what measures are being generated, and hiwhc researchers are the experts.


<h7>High level objective </h7>
<p> Use NLP to automate the discovery of how scientific data are referenced in publications. </p>

<h7>Data </h7>
<p>Given an publication (text data) find the correct dataset labels (text data). So, a set of arrays (that represent the words in data), to another set of arrays. </p>

<h7>How do we solve this problem?</h7>
<p>Should we use a pretrained model or build a new one? 
	- I think it is better to first build a new model, to further increase my knowledge in ML.
	- Then later try to use a pre-trained model, like Roberta. Because it's a more advanced stuff, when you have better understanding of ML and pre-trained models. It's better to learn the basics before doing advanced stuff.</p>

<p>How do we represent the data? Either by arrays or GloVe's</p>
	- I think that Glove's are a better choice, upon some quikc research they are more efficient. Keras also have built in layers for embedding (Embedding layers).


<strong> Next Todo</strong>
Take a subset of the data and try to create a simpler model.

